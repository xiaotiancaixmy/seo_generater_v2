import os
import csv
from jinja2 import Template
from openai import OpenAI
from config import OPENROUTER_API_KEY, MODEL_NAME
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from collections import Counter
import re

# â”€â”€â”€â”€â”€â”€ åˆå§‹åŒ– OpenRouter å®¢æˆ·ç«¯ â”€â”€â”€â”€â”€â”€
client = OpenAI(
    api_key=OPENROUTER_API_KEY,
    base_url="https://openrouter.ai/api/v1"
)

# â”€â”€â”€â”€â”€â”€ å¸¸é‡é…ç½® â”€â”€â”€â”€â”€â”€
DATA_FILE = "data/titles.csv"
OUTLINE_TEMPLATE = "prompts/outline_template.txt"
ARTICLE_TEMPLATE = "prompts/article_template.txt"
OUTPUT_DIR = "outputs"
TARGET_WORDS = 2000
MAX_TOKENS = 5000
SIMILARITY_THRESHOLD = 0.8  # ç›¸ä¼¼åº¦é˜ˆå€¼

os.makedirs(OUTPUT_DIR, exist_ok=True)

# â”€â”€â”€â”€â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€

def chat(prompt, temperature=0.7, max_tokens=MAX_TOKENS):
    """ç»Ÿä¸€çš„å¯¹è¯è¯·æ±‚å‡½æ•°"""
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature,
        max_tokens=max_tokens
    )
    return response.choices[0].message.content.strip()

def load_titles():
    """åŠ è½½ CSV æ–‡ä»¶ä¸­çš„æ‰€æœ‰è¡Œï¼Œè¿”å›å­—å…¸åˆ—è¡¨"""
    with open(DATA_FILE, newline='', encoding='utf-8') as f:
        return list(csv.DictReader(f))

def get_main_keyword(title_row: dict) -> str:
    """
    ä» title_rowï¼ˆCSV çš„ä¸€è¡Œå­—å…¸ï¼‰ä¸­ç›´æ¥æå– key_words å­—æ®µ
    """
    return title_row.get('key_words', '').strip().lower()

def web_search_summary(keyword: str) -> str:
    prompt = f"""Search the web and provide a comprehensive summary of recent content related to: {keyword}

Requirements:
1. First list the top 3-5 most relevant web sources you found
2. Then provide a detailed summary of the information from these sources
3. Include key statistics, trends, and insights
4. Focus on recent and authoritative information
5. Format the output as follows:

SOURCES:
- [Source 1]
- [Source 2]
- [Source 3]

SUMMARY:
[Your detailed summary here]"""
    return chat(prompt, temperature=0.3)

def generate_outline(summary: str, keyword: str) -> str:
    with open(OUTLINE_TEMPLATE, encoding='utf-8') as f:
        template = Template(f.read())
    prompt = template.render(summary=summary, keyword=keyword)
    return chat(prompt, temperature=0.3)

def check_content_duplication(text: str) -> tuple[bool, list[str]]:
    """æ£€æŸ¥å†…å®¹é‡å¤
    è¿”å›: (æ˜¯å¦æœ‰é‡å¤, é‡å¤å†…å®¹åˆ—è¡¨)
    """
    # å°†æ–‡æœ¬åˆ†æˆæ®µè½
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    
    # æ£€æŸ¥æ®µè½çº§åˆ«çš„é‡å¤
    duplicates = []
    for i, p1 in enumerate(paragraphs):
        for j, p2 in enumerate(paragraphs[i+1:], i+1):
            # è®¡ç®—ä¸¤ä¸ªæ®µè½çš„ç›¸ä¼¼åº¦
            similarity = calculate_similarity(p1, p2)
            if similarity > SIMILARITY_THRESHOLD:
                duplicates.append(f"æ®µè½ {i+1} å’Œ {j+1} ç›¸ä¼¼åº¦: {similarity:.2f}")
    
    return len(duplicates) > 0, duplicates

def calculate_similarity(text1: str, text2: str) -> float:
    """è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
    # ä½¿ç”¨ç®€å•çš„è¯é¢‘æ¯”è¾ƒ
    words1 = set(re.findall(r'\w+', text1.lower()))
    words2 = set(re.findall(r'\w+', text2.lower()))
    
    if not words1 or not words2:
        return 0.0
    
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    return len(intersection) / len(union)

def optimize_content(text: str) -> str:
    """ä¼˜åŒ–å†…å®¹ï¼Œå»é™¤é‡å¤å¹¶é‡å†™"""
    has_duplicates, duplicates = check_content_duplication(text)
    
    if has_duplicates:
        print("âš ï¸ æ£€æµ‹åˆ°å†…å®¹é‡å¤ï¼Œæ­£åœ¨ä¼˜åŒ–...")
        prompt = f"""
        ä»¥ä¸‹æ–‡ç« å­˜åœ¨å†…å®¹é‡å¤ï¼Œè¯·é‡å†™ä»¥æ¶ˆé™¤é‡å¤ï¼ŒåŒæ—¶ä¿æŒæ–‡ç« çš„æ•´ä½“ç»“æ„å’Œè´¨é‡ï¼š
        
        {text}
        
        é‡å¤å†…å®¹ï¼š
        {chr(10).join(duplicates)}
        
        è¦æ±‚ï¼š
        1. ä¿æŒæ–‡ç« çš„ä¸»è¦è§‚ç‚¹å’Œä¿¡æ¯
        2. ä½¿ç”¨ä¸åŒçš„è¡¨è¾¾æ–¹å¼é‡å†™é‡å¤çš„éƒ¨åˆ†
        3. ç¡®ä¿æ–‡ç« æµç•…è‡ªç„¶
        4. ä¿æŒåŸæœ‰çš„æ®µè½ç»“æ„
        5. ä¸è¦æ”¹å˜æ–‡ç« çš„ä¸»é¢˜å’Œç›®çš„
        """
        return chat(prompt, temperature=0.7)
    
    return text

def first_draft_article(title: str, keyword: str, outline: str, summary: str) -> str:
    """ç”Ÿæˆæ–‡ç« åˆç¨¿"""
    with open(ARTICLE_TEMPLATE, encoding="utf-8") as f:
        template = Template(f.read())
    prompt = template.render(
        title=title,
        main_keyword=keyword,
        outline=outline,
        search_summary=summary
    )
    return chat(prompt, temperature=0.7)

def expand_section(section: str, target_words: int) -> str:
    """æ‰©å†™ç‰¹å®šæ®µè½"""
    prompt = f"""
    Expand the following section to approximately {target_words} words while maintaining its original meaning and structure:
    
    {section}
    
    Requirements:
    - Keep the same tone and style
    - Add relevant examples and details
    - Maintain paragraph structure
    - Ensure natural flow
    - Avoid repeating information
    """
    return chat(prompt, temperature=0.7)

def ensure_faq_section(article: str) -> str:
    """ç¡®ä¿æ–‡ç« åŒ…å«FAQéƒ¨åˆ†"""
    if "FAQ" not in article and "Frequently Asked Questions" not in article:
        prompt = f"""
        Add a comprehensive FAQ section to this article with 5-7 common questions and detailed answers:
        
        {article}
        
        Requirements:
        - Add 5-7 relevant questions
        - Provide detailed, helpful answers
        - Use natural language
        - Include examples where appropriate
        - Ensure questions are unique and not repetitive
        """
        return chat(prompt, temperature=0.7)
    return article

def save_article(article_id: str, content: str):
    """ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶"""
    path = os.path.join(OUTPUT_DIR, f"article-{article_id.zfill(3)}.md")
    with open(path, "w", encoding='utf-8') as f:
        f.write(content)

def main():
    rows = load_titles()
    for idx, row in enumerate(rows, start=1):
        article_id = str(idx)
        keyword = get_main_keyword(row)
        print(f"\nğŸ” Processing: {row['title']}")
        
        # 1. ç”Ÿæˆæ‘˜è¦å’Œæçº²
        summary = web_search_summary(keyword)
        print(f"the keyword is {keyword}")
        print("âœ… Summary done.")
        
        outline = generate_outline(summary, keyword)
        print(f"the outline is {outline}")
        print("âœ… Outline generated.")
        
        # 2. ç”Ÿæˆåˆç¨¿
        article = first_draft_article(row['title'], keyword, outline, summary)
        print(f"   â†³ Draft length: {len(article.split())} words")
        
        # 3. ç¡®ä¿æ–‡ç« é•¿åº¦
        while len(article.split()) < TARGET_WORDS:
            # åˆ†ææ–‡ç« ç»“æ„ï¼Œæ‰¾å‡ºæœ€çŸ­çš„éƒ¨åˆ†è¿›è¡Œæ‰©å†™
            sections = article.split('\n\n')
            shortest_section = min(sections, key=lambda x: len(x.split()))
            expanded_section = expand_section(shortest_section, 300)
            article = article.replace(shortest_section, expanded_section)
            print(f"   â†³ Extended to {len(article.split())} words")
        
        # 4. ç¡®ä¿åŒ…å«FAQéƒ¨åˆ†
        article = ensure_faq_section(article)
        print("âœ… FAQ section added/verified")
        
        # 5. æ£€æŸ¥å¹¶ä¼˜åŒ–é‡å¤å†…å®¹
        article = optimize_content(article)
        print("âœ… Content optimized")
        
        # 6. ä¿å­˜æ–‡ç« 
        save_article(article_id, article)
        print(f"âœ… Article saved to outputs/article-{article_id.zfill(3)}.md")

if __name__ == "__main__":
    main()

    
'''
def first_draft_article(title: str, keyword: str, outline: str, summary: str) -> str:
    with open(ARTICLE_TEMPLATE, encoding="utf-8") as f:
        template = Template(f.read())
    prompt = template.render(
        title=title,
        main_keyword=keyword,
        outline=outline,
        search_summary=summary
    )
    return chat(prompt, temperature=0.7)
def continue_article(existing: str, title: str) -> str:
    prompt = (
        f"The article titled '{title}' is currently {len(existing.split())} words. "
        f"Carefully expand only the existing sections of the article that are less than 150 words. Keep the original paragraph structure, order, tone, and intent unchanged. Do not introduce new paragraphs, rearrange content, or continue the article. Simply enrich short paragraphs with more depth, examples, or clarity. Continue this process only until the total word count of the article exceeds {TARGET_WORDS} words.Ensure that the outline and section headings remain unchanged. Do not split or merge paragraphs. Only add to short paragraphs to better meet the expected depth and coverage."
    )
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": "Here is the article so far:"},
            {"role": "user", "content": existing},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=2048
    )
    return response.choices[0].message.content.strip()

def save_md(article_id: str, text: str):
    path = os.path.join(OUTPUT_DIR, f"article-{article_id.zfill(3)}.md")
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)

# â”€â”€â”€â”€â”€â”€ ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€

def main():
    titles = load_titles()
    for row in titles:
        title = row["title"]
        article_id = row["id"]
        keyword = get_main_keyword(title)

        print(f"\nğŸ” Processing: {title}")

        summary = web_search_summary(keyword)
        print("âœ… Summary done.")

        outline = generate_outline(summary, keyword)
        print("âœ… Outline generated.")

        article = first_draft_article(title, keyword, outline, summary)
        print(f"   â†³ Draft length: {len(article.split())} words")

        while len(article.split()) < TARGET_WORDS:
            extra = continue_article(article, title)
            article += "\n\n" + extra
            print(f"   â†³ Extended to {len(article.split())} words")

        save_md(article_id, article)
        print("âœ…  Saved â†’", f"outputs/article-{article_id.zfill(3)}.md")

if __name__ == "__main__":
    main()
'''